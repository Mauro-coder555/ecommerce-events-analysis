{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"MiAplicacion\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "csv_files = [os.path.join(\"archive\", f) for f in os.listdir(\"archive\") if f.endswith('.csv')]\n",
    "\n",
    "df = spark.read.options(header='True', inferSchema='True').csv(csv_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+-----------+-------------+-------+-----+-------+------------+\n",
      "|event_time|event_type|product_id|category_id|category_code|  brand|price|user_id|user_session|\n",
      "+----------+----------+----------+-----------+-------------+-------+-----+-------+------------+\n",
      "|         0|         0|         0|          0|     20339246|8757117|    0|      0|        4598|\n",
      "+----------+----------+----------+-----------+-------------+-------+-----+-------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Muestra conteo de nulos por columna\n",
    "df.select([F.count(F.when(F.isnull(c), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputamos los valores nulos ya que las columnas en cuestion no son críticas.\n",
    "df_clean = df.fillna({\n",
    "    \"category_code\": \"unknown\",\n",
    "    \"brand\": \"unknown\"\n",
    "})\n",
    "\n",
    "# Al ser pocas filas, optamos por eliminarlas. \n",
    "df_clean = df_clean.filter(F.col(\"user_session\").isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+-----------+-------------+-----+-----+-------+------------+\n",
      "|event_time|event_type|product_id|category_id|category_code|brand|price|user_id|user_session|\n",
      "+----------+----------+----------+-----------+-------------+-----+-----+-------+------------+\n",
      "|         0|         0|         0|          0|            0|    0|    0|      0|           0|\n",
      "+----------+----------+----------+-----------+-------------+-----+-----+-------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verificación\n",
    "\n",
    "df_clean.select([F.count(F.when(F.isnull(c), c)).alias(c) for c in df_clean.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------+---------+---------+\n",
      "|       avg_price|         std_price|min_price|max_price|\n",
      "+----------------+------------------+---------+---------+\n",
      "|8.53489224700748|19.382060565203542|   -79.37|   327.78|\n",
      "+----------------+------------------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Estadísticas básicas de precio\n",
    "df_clean.select(\n",
    "    F.mean(\"price\").alias(\"avg_price\"),\n",
    "    F.stddev(\"price\").alias(\"std_price\"),\n",
    "    F.min(\"price\").alias(\"min_price\"),\n",
    "    F.max(\"price\").alias(\"max_price\")\n",
    ").show()\n",
    "\n",
    "# Filtrar precios \"razonables\" (ej: entre 0.01 y 10,000)\n",
    "df_clean = df_clean.filter((F.col(\"price\") > 0.01) & (F.col(\"price\") < 10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos precios negativos, podrían ser reembolsos, cupones etc. Pero deberia estar especificado de otra forma.\n",
    "\n",
    "df_clean = df_clean.filter(F.col(\"price\") >= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------------+---------+---------+\n",
      "|        avg_price|         std_price|min_price|max_price|\n",
      "+-----------------+------------------+---------+---------+\n",
      "|9.377097434957705|20.792348796755153|     0.05|   327.78|\n",
      "+-----------------+------------------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Estadísticas básicas de precio\n",
    "df_clean.select(\n",
    "    F.mean(\"price\").alias(\"avg_price\"),\n",
    "    F.stddev(\"price\").alias(\"std_price\"),\n",
    "    F.min(\"price\").alias(\"min_price\"),\n",
    "    F.max(\"price\").alias(\"max_price\")\n",
    ").show()\n",
    "\n",
    "# Filtrar precios \"razonables\" (ej: entre 0.01 y 10,000)\n",
    "df_clean = df_clean.filter((F.col(\"price\") > 0.01) & (F.col(\"price\") < 10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Límite superior (IQR): 15.75, Percentil 99%: 327.78\n"
     ]
    }
   ],
   "source": [
    "quantiles = df_clean.approxQuantile(\"price\", [0.25, 0.75, 0.99], 0.01)\n",
    "Q1, Q3, P99 = quantiles[0], quantiles[1], quantiles[2]\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "upper_limit = Q3 + 1.5 * IQR\n",
    "print(f\"Límite superior (IQR): {upper_limit}, Percentil 99%: {P99}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pesar de que puede ser extraño, comprendemos que puede haber cosmeticos de hasta mas de 300 dolares. Por lo que no tomamos medidas respecto a valores atipicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+----------+-------------------+-------------+------+-----+---------+--------------------+\n",
      "|         event_time|event_type|product_id|        category_id|category_code| brand|price|  user_id|        user_session|\n",
      "+-------------------+----------+----------+-------------------+-------------+------+-----+---------+--------------------+\n",
      "|2019-11-14 16:54:04|      view|   5635474|1487580008984608779|      unknown|entity|307.6|571598796|18af287e-6279-41c...|\n",
      "|2019-10-07 10:00:58|      view|   5635474|1487580008984608779|      unknown|entity|307.6|440589741|726655f1-fa53-42e...|\n",
      "|2019-10-09 14:50:11|      view|   5635474|1487580008984608779|      unknown|entity|307.6|558545066|464127c4-1cfb-423...|\n",
      "|2019-10-09 18:43:02|      view|   5635474|1487580008984608779|      unknown|entity|307.6|480150376|72b5f64a-618c-4a8...|\n",
      "|2019-10-15 09:50:40|      view|   5635474|1487580008984608779|      unknown|entity|307.6|560482622|583ea02a-fd3f-44d...|\n",
      "|2019-10-19 15:10:14|      view|   5635474|1487580008984608779|      unknown|entity|307.6|562007569|7191f2ea-aae7-4bc...|\n",
      "|2020-02-17 05:00:56|      view|   5635474|1487580008984608779|      unknown|entity|307.6|615750758|0441c076-f6a5-424...|\n",
      "|2020-02-17 16:40:42|      view|   5635474|1487580008984608779|      unknown|entity|307.6|540253593|c2d2034a-f498-47e...|\n",
      "|2020-02-17 16:41:31|      view|   5635474|1487580008984608779|      unknown|entity|307.6|540253593|c2d2034a-f498-47e...|\n",
      "|2020-02-20 13:45:53|      view|   5635474|1487580008984608779|      unknown|entity|307.6|568290584|ea58d00a-17fd-494...|\n",
      "|2020-02-20 13:46:25|      view|   5635474|1487580008984608779|      unknown|entity|307.6|568290584|ea58d00a-17fd-494...|\n",
      "|2020-02-20 13:46:34|      view|   5635474|1487580008984608779|      unknown|entity|307.6|568290584|ea58d00a-17fd-494...|\n",
      "+-------------------+----------+----------+-------------------+-------------+------+-----+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_clean.filter(F.col(\"price\")>300).filter((F.col(\"brand\") != \"strong\")  & (F.col(\"brand\") != \"unknown\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usuarios con >100 eventos/día: 14795\n",
      "+---------+--------------+\n",
      "|  user_id|events_per_day|\n",
      "+---------+--------------+\n",
      "|343320891|           171|\n",
      "|414453512|           102|\n",
      "|438606371|           106|\n",
      "|479888694|           143|\n",
      "|483574352|           183|\n",
      "|495786914|           123|\n",
      "|527692709|           106|\n",
      "|527739278|           201|\n",
      "|550355871|           181|\n",
      "|563408172|           266|\n",
      "+---------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Agregar columna de eventos por día\n",
    "user_daily_events = df_clean.withColumn(\n",
    "    \"events_per_day\", \n",
    "    F.count(\"*\").over(Window.partitionBy(\"user_id\", F.to_date(\"event_time\")))\n",
    ")\n",
    "\n",
    "# Filtrar usuarios con >100 eventos/día y contar ocurrencias\n",
    "high_activity_users = user_daily_events.filter(F.col(\"events_per_day\") > 100)\n",
    "print(f\"Usuarios con >100 eventos/día: {high_activity_users.select('user_id').distinct().count()}\")\n",
    "high_activity_users.select(\"user_id\", \"events_per_day\").distinct().show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usuarios con >100 eventos/día: 1636982\n",
      "+---------+--------------+\n",
      "|  user_id|events_per_day|\n",
      "+---------+--------------+\n",
      "| 31156111|             1|\n",
      "| 34915661|             1|\n",
      "| 67944478|             1|\n",
      "|152961343|             1|\n",
      "|153316955|            17|\n",
      "|196042408|             4|\n",
      "|204142009|             1|\n",
      "|204166748|             7|\n",
      "|223293700|            28|\n",
      "|228684512|             2|\n",
      "+---------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filtrar usuarios con >100 eventos/día y contar ocurrencias\n",
    "high_activity_users = user_daily_events.filter(F.col(\"events_per_day\") < 100)\n",
    "print(f\"Usuarios con >100 eventos/día: {high_activity_users.select('user_id').distinct().count()}\")\n",
    "high_activity_users.select(\"user_id\", \"events_per_day\").distinct().show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a tomar una decision sensata sin mayores complicaciones por el momento. Hay aproximadamente 1.6 millones de usuarios, de los cuales 15 mil usuarios tienen mas de 100 sesiones por dia. Vamos a eliminar los registros con mas de 150 sesiones, con el fin de preservar algun posible cliente extremo o revendedor pero limitando la posibilidad de bots obvios en la plataforma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Calcular eventos diarios por usuario\n",
    "df_clean = df_clean.withColumn(\n",
    "    \"events_per_day\", \n",
    "    F.count(\"*\").over(Window.partitionBy(\"user_id\", F.to_date(\"event_time\")))\n",
    ")\n",
    "\n",
    "# 2. Filtrar SOLO registros donde eventos_per_day < 150\n",
    "df_clean = df_clean.filter(F.col(\"events_per_day\") < 150).drop(\"events_per_day\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+-------------+\n",
      "|        user_session|         start_time|           end_time|duration_days|\n",
      "+--------------------+-------------------+-------------------+-------------+\n",
      "|2c1569d4-8ab3-414...|2019-10-01 11:12:49|2020-02-29 10:50:25|          151|\n",
      "|52b30a79-923b-461...|2019-10-01 06:07:13|2020-02-29 08:06:45|          151|\n",
      "|ac7b1c23-10a7-485...|2019-10-01 17:32:33|2020-02-29 18:34:01|          151|\n",
      "|38785db2-b8d6-4c3...|2019-10-01 15:05:11|2020-02-29 13:30:03|          151|\n",
      "|ae74cec4-ae31-447...|2019-10-01 02:48:54|2020-02-29 17:34:50|          151|\n",
      "|1345d1ab-4163-46c...|2019-10-01 18:40:21|2020-02-29 07:54:17|          151|\n",
      "|099fefe4-a74c-4da...|2019-10-01 03:52:13|2020-02-29 14:31:51|          151|\n",
      "|5b9bcf07-5c80-4f9...|2019-10-01 03:34:49|2020-02-29 13:15:36|          151|\n",
      "|32d76835-fa23-4e5...|2019-10-01 16:06:58|2020-02-29 14:44:47|          151|\n",
      "|b8ec8ea3-5fbd-40e...|2019-10-01 02:14:54|2020-02-29 02:08:35|          151|\n",
      "+--------------------+-------------------+-------------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import datediff\n",
    "\n",
    "session_duration = df_clean.groupBy(\"user_session\").agg(\n",
    "    F.min(\"event_time\").alias(\"start_time\"),\n",
    "    F.max(\"event_time\").alias(\"end_time\"),\n",
    "    datediff(F.max(\"event_time\"), F.min(\"event_time\")).alias(\"duration_days\")\n",
    ")\n",
    "\n",
    "# Top 10 sesiones más largas\n",
    "session_duration.orderBy(F.desc(\"duration_days\")).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos una cantidad anomala de dias de duracion en algunas sesiones, algo que no es posible. Una sesion no puede durar 5 meses. Vamos a explorar la cantidad de sesiones por rangos de dias mas aceptables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------+--------+---------+--------------+\n",
      "|<15_days|>15_days|>30_days|>50_days|>100_days|total_sessions|\n",
      "+--------+--------+--------+--------+---------+--------------+\n",
      "| 4356772|    9911|    7265|    5697|     1904|       4366938|\n",
      "+--------+--------+--------+--------+---------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count, when\n",
    "\n",
    "# Calcular la duración en días de cada sesión (si no lo has hecho)\n",
    "session_duration = df_clean.groupBy(\"user_session\").agg(\n",
    "    (F.datediff(F.max(\"event_time\"), F.min(\"event_time\"))).alias(\"duration_days\")\n",
    ")\n",
    "\n",
    "# Contar sesiones en cada rango\n",
    "duration_bins = session_duration.select(\n",
    "    F.count(F.when(F.col(\"duration_days\") < 15, True)).alias(\"<15_days\"),\n",
    "    F.count(F.when(F.col(\"duration_days\") > 15, True)).alias(\">15_days\"),\n",
    "    F.count(F.when(F.col(\"duration_days\") > 30, True)).alias(\">30_days\"),\n",
    "    F.count(F.when(F.col(\"duration_days\") > 50, True)).alias(\">50_days\"),\n",
    "    F.count(F.when(F.col(\"duration_days\") > 100, True)).alias(\">100_days\"),\n",
    "    F.count(\"*\").alias(\"total_sessions\")\n",
    ").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
